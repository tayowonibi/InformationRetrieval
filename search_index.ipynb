{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mowonibi\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from enum import Enum\n",
    "import math\n",
    "import pickle\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 100\n",
    "WORD2VEC_FILE=\"C:\\\\Users\\\\mowonibi\\\\Downloads\\\\crawl-300d-2M.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryTermMode(Enum): \n",
    "    \"\"\" \n",
    "    Enum Class :\n",
    "    Enumeration of functions to to use for the mode in combining the results of each query term\n",
    "    \"\"\"\n",
    "    OR = set.union # all documents containing any of the query terms should be returned\n",
    "    AND = set.intersection #only documents containing all of the query terms to be returned\n",
    "\n",
    "class Helper:\n",
    "    \"\"\"\n",
    "    Class containing any helper function to be used in the application\n",
    "    \"\"\"\n",
    "    \n",
    "    tknzr = TweetTokenizer()\n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    @staticmethod\n",
    "    def custom_tokenizer( sentence):\n",
    "        \"\"\"\n",
    "        lemmatize lower case tokens of sentence tokenized using nltk tweet tokenizer\n",
    "        Returns list of tokens\n",
    "        \"\"\"\n",
    "        return [WordNetLemmatizer().lemmatize(token.lower() ) for token in Helper.tknzr.tokenize(sentence) if token not in Helper.en_stop and len(token) > 1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def simple_spliter_tokenizer(sentence, splitter=\"/\"):\n",
    "        \"\"\"\n",
    "        simple splitter based on user-defined regex expressions\n",
    "        Returns list of tokens\n",
    "        \"\"\"\n",
    "        return [token.lower()  for token in sentence.split(splitter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankFunction :\n",
    "    \n",
    "    @staticmethod\n",
    "    def avg_embedding_similarity(inverted_index, doc_set =None):\n",
    "        \"\"\"\n",
    "        Computes the BM25 scores of document given a query strinng and the query field\n",
    "        Returns dictionary of document ids and BM25 scores\n",
    "        \"\"\"\n",
    "\n",
    "        def avg_embedding_similarity_ranking(field_name, query_terms_as_string):\n",
    "            docScore  =defaultdict(float)\n",
    "            tokenized_text = inverted_index.field_names[field_name](query_terms_as_string)\n",
    "            query_emb =inverted_index.get_avg_doc_embedding(tokenized_text)\n",
    "            docs_wth_term =set()\n",
    "            for word in tokenized_text:\n",
    "                docs_wth_term =set.union(docs_wth_term, inverted_index.term_frequency[field_name][word].keys())\n",
    "            #print(len(docs_wth_term))\n",
    "            #print(query_emb)\n",
    "            for doc_id in docs_wth_term:\n",
    "                #print(doc_id)\n",
    "                d_scor = 1- cosine_similarity(query_emb.reshape( 1,300), inverted_index.doc_avg_embeddings[field_name][doc_id].reshape( 1,300))[0]\n",
    "                docScore [doc_id] = d_scor\n",
    "                #print(doc_id, d_scor)\n",
    "            return docScore\n",
    "        return avg_embedding_similarity_ranking\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def lda_similarity(inverted_index, doc_set =None):\n",
    "        \"\"\"\n",
    "        Computes the BM25 scores of document given a query strinng and the query field\n",
    "        Returns dictionary of document ids and BM25 scores\n",
    "        \"\"\"\n",
    "        def lda_similarity_ranking(field_name, query_terms_as_string):\n",
    "            docScore  =defaultdict(float)\n",
    "            tokenized_text = inverted_index.field_names[field_name](query_terms_as_string)\n",
    "            #query_emb =inverted_index.get_avg_doc_embedding(tokenized_text)\n",
    "            new_doc_bow = inverted_index.field_dict[field_name].doc2bow(tokenized_text)\n",
    "            ldamodel=inverted_index.field_model[field_name]\n",
    "            doc_topic = np.zeros((1,100))\n",
    "            topic_tuple_list =ldamodel.get_document_topics(new_doc_bow)\n",
    "            #print(topic_tuple_list)\n",
    "            for top_ind,val in topic_tuple_list:\n",
    "                doc_topic[0,top_ind]=val  \n",
    "            docs_wth_term =set()\n",
    "            for word in tokenized_text:\n",
    "                docs_wth_term =set.union(docs_wth_term, inverted_index.term_frequency[field_name][word].keys())\n",
    "            \n",
    "            for doc_id in docs_wth_term:\n",
    "                \n",
    "                new_doc_bowx = inverted_index.field_dict[field_name].doc2bow(inverted_index.document_tokens[field_name][doc_id])\n",
    "                doc_topicx = np.zeros((1,100))\n",
    "                for top_ind,val in ldamodel.get_document_topics(new_doc_bowx):\n",
    "                    doc_topicx[0,top_ind]=val\n",
    "                \n",
    "                \n",
    "                d_scor = 1- cosine_similarity(doc_topicx,doc_topic )[0]\n",
    "                docScore [doc_id] = d_scor\n",
    "                #print(doc_id, d_scor)\n",
    "            return docScore\n",
    "        return lda_similarity_ranking\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def BM25_score(inverted_index, doc_set =None, PARAM_K1=1.2, PARAM_B=0.75, EPSILON=0.25):\n",
    "        \"\"\"\n",
    "        Computes the BM25 scores of document given a query strinng and the query field\n",
    "        Returns dictionary of document ids and BM25 scores\n",
    "        \"\"\"\n",
    "        #https://github.com/nhirakawa/BM25/blob/master/src/rank.py - consider this implementation\n",
    "        #https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables\n",
    "        #https://www.quora.com/How-does-BM25-work\n",
    "        #https://github.com/nhirakawa/BM25/blob/master/src/query.py\n",
    "        \n",
    "        def bm25_ranking(field_name, query_terms_as_string):\n",
    "            docScore  =defaultdict(float)\n",
    "            tokenized_text = inverted_index.field_names[field_name](query_terms_as_string)\n",
    "            for word in tokenized_text:\n",
    "                docs_wth_term =inverted_index.term_frequency[field_name][word]\n",
    "                for doc, qtf in docs_wth_term.items():\n",
    "                    #print(self.doc_index.avg_doc_len[field_name], self.doc_index.document_length[field_name][doc])\n",
    "                    idf = inverted_index.idf[field_name][word] #if self.doc_index.idf[field_name][word] >= 0 else EPSILON * self.doc_index.average_idf[field_name]\n",
    "                    docScore[doc] += (idf * qtf * (PARAM_K1 + 1)\n",
    "                      / (qtf + PARAM_K1 * (1 - PARAM_B + PARAM_B * inverted_index.document_length[field_name][doc] / inverted_index.avg_doc_len[field_name])))\n",
    "            return docScore\n",
    "        return bm25_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inverted_Index:\n",
    "    \"\"\"indexer class for managing the index i.e. adding index, removing index etc\"\"\"\n",
    "    \n",
    "    def __init__(self, file_location=\"products/products.csv\",field_names={\"category\" :Helper.simple_spliter_tokenizer ,\"title\":Helper.custom_tokenizer,\"description\":Helper.custom_tokenizer}):\n",
    "        self.file_location = file_location #file to index\n",
    "        self.field_names  = field_names #This contains the set of field names used in the index and their tokenizer\n",
    "        self.term_frequency =defaultdict( lambda: defaultdict(lambda: defaultdict(int)) ) # maps/dict of dict/maps of field -> term ->docs-> freq of occurence in doc\n",
    "        self.document_tokens = defaultdict( lambda: defaultdict(list) ) #field  ->docs ->terms\n",
    "        self.idf =defaultdict (lambda: dict() )#idf of field-> term-> idf\n",
    "        self.inverted_index_term_proximity = defaultdict( lambda:defaultdict(lambda: defaultdict(list))) #field ->term ->doc ->list of positions in document\n",
    "        self.corpus =None #pandas dataframe of the products \n",
    "        self.document_length=defaultdict( lambda:defaultdict(lambda: defaultdict(int))) #field -> document_id -> length of document\n",
    "        self.num_of_doc=0 #total number of doc /products\n",
    "        self.avg_doc_len = dict() #field -> average document length\n",
    "        self.avg_idf = dict() #field -> avg idf\n",
    "        self.field_dict=dict()\n",
    "        self.field_corpus=dict()\n",
    "        self.field_model=dict()\n",
    "        self.term_embeddings= dict() #term-> embedding  #defaultdict(lambda: np.zeros((300,))) \n",
    "        self.doc_avg_embeddings=defaultdict(lambda: defaultdict(lambda: np.zeros((300,)))) # field ->doc ->embedding\n",
    "    \n",
    "    def index_texts(self, file_location=None, include_doc2vec=False, index_doc_embedding=False):\n",
    "        \"\"\"indexs the files in the text\"\"\"\n",
    "        sum_of_doc_len=defaultdict(int) #field ->sum of document lenght  which can be used to compute average document length lateron\n",
    "        field_doc_tokens=defaultdict(lambda: defaultdict(list)) #field->doc->tokens\n",
    "        if file_location is None:\n",
    "            file_location = self.file_location\n",
    "        self.corpus = pd.read_csv(file_location)\n",
    "        \n",
    "        for index, row in self.corpus.iterrows(): #loop rows of products\n",
    "            self.num_of_doc += 1\n",
    "            #if (self.num_of_doc >20):\n",
    "            #    break\n",
    "            for field_name, tokenizer  in self.field_names.items():   #loops each field\n",
    "                tokenized_text = tokenizer(row[field_name])\n",
    "                if include_doc2vec :\n",
    "                    field_doc_tokens[field_name][index]=tokenized_text\n",
    "                current_doc_len= len(tokenized_text)\n",
    "                self.document_length[field_name][index] =current_doc_len\n",
    "                sum_of_doc_len[field_name] +=current_doc_len\n",
    "                self.document_tokens[field_name][index]=tokenized_text\n",
    "\n",
    "                for k,word in enumerate(tokenized_text):     #loops each tokenized text\n",
    "                    self.inverted_index_term_proximity[field_name][word][index].append( k)\n",
    "                    self.term_frequency[field_name] [word][index] += 1\n",
    "\n",
    "        sum_of_idf=defaultdict(float) #field -> sum of the idf\n",
    "        #computes the idf of each term in the corpus\n",
    "        for field_name, field_tf in self.term_frequency.items(): \n",
    "            for word, tf in field_tf.items():\n",
    "                self.idf[field_name][word] = math.log(self.num_of_doc - len(tf) + 0.5) - math.log(len(tf) + 0.5)\n",
    "                sum_of_idf[field_name]+=self.idf[field_name][word] #aggregate/sums the idf by summing to the aggregator\n",
    "        \n",
    "        for field_name in self.field_names:\n",
    "            self.avg_doc_len[field_name] = sum_of_doc_len[field_name] / self.num_of_doc\n",
    "            self.avg_idf[field_name] = sum_of_idf[field_name] / self.num_of_doc\n",
    "        \n",
    "        if include_doc2vec :\n",
    "            self._index_doc2vec(  field_doc_tokens)\n",
    "        \n",
    "        if index_doc_embedding:\n",
    "            self.index_doc_avg_embedding()\n",
    "    \n",
    "    def _index_doc2vec(self,  field_doc_tokens):\n",
    "        #field_corpus =dict()\n",
    "        #field_dict =dict()\n",
    "        for field_name, _  in self.field_names.items():\n",
    "            dictionary = corpora.Dictionary([list(self.term_frequency[field_name].keys())])\n",
    "            t_documents = [dictionary.doc2bow(text) for  text in  field_doc_tokens[field_name].values()]  #create bog of words (multi-set)\n",
    "            #model = gensim.models.LdaMulticore(t_documents, num_topics = NUM_TOPICS, id2word=dictionary, passes=30, workers=3)\n",
    "            #model.save(field_name +'_model100.gensim')\n",
    "            #dictionary.save(field_name +'_dictionary.gensim')\n",
    "            dictionary = Dictionary.load(field_name_+'dictionary.gensim')\n",
    "            model = LdaModel.load(field_name_ +\"model100.gensim\", mmap='r')\n",
    "            self.field_dict[field_name]=dictionary\n",
    "            self.field_corpus[field_name]=t_documents\n",
    "            self.field_model[field_name]=model\n",
    "            \n",
    "    def index_doc_avg_embedding (self):\n",
    "        self.load_term_embedding()\n",
    "        for field_name  in self.field_names.keys():\n",
    "            for doc_id  in self.document_tokens[field_name].keys():\n",
    "                self.doc_avg_embeddings[field_name][doc_id] = self.get_avg_doc_embedding(self.document_tokens[field_name][doc_id]) \n",
    "                   \n",
    "                \n",
    "    def get_avg_doc_embedding (self, tokens):\n",
    "        embedding_accum = np.zeros((300,))\n",
    "        d_len=0\n",
    "        for tkn in tokens:\n",
    "            if (tkn in self.term_embeddings.keys()):\n",
    "                embedding_accum +=self.term_embeddings[tkn]\n",
    "                d_len+=1\n",
    "        return embedding_accum/d_len\n",
    "\n",
    "    def  load_term_embedding(self):\n",
    "        term_set= set()\n",
    "        for field_name in self.field_names.keys():\n",
    "            term_set = set.union(term_set,self.term_frequency[field_name].keys() )\n",
    "        #print(term_set)\n",
    "        with open(WORD2VEC_FILE, encoding='utf8') as f:\n",
    "            next(f)\n",
    "            for l in f:\n",
    "                w = l.split(' ')\n",
    "                if w[0] in term_set:\n",
    "                    self.term_embeddings[w[0]] = np.array([float(x) for x in w[1:301]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher:\n",
    "    \"\"\"class for searching document based on a given index\"\"\"\n",
    "    def __init__(self, doc_index ):\n",
    "        self.doc_index = doc_index\n",
    "        \n",
    "    def field_term_search(self, field_name, query_tokenized_terms_list, max_separation_between_terms = None, q_mode=QueryTermMode.OR) : #max_separation_between_terms removes effect of q_mode\n",
    "        \"\"\"\n",
    "        search list of tokens, in a specific field, given query mode and max separation between terms in a given document\n",
    "        Returns list of set of doucment/product id\n",
    "        \"\"\"\n",
    "        \n",
    "        result_set =None\n",
    "        prev_result_set=None\n",
    "        if max_separation_between_terms is None :\n",
    "            for q_term in query_tokenized_terms_list:\n",
    "                result_set =set(self.doc_index.inverted_index_term_proximity[field_name][q_term].keys()) if result_set is None else q_mode(result_set, set(self.doc_index.inverted_index_term_proximity[field_name][q_term].keys()))\n",
    "        else :\n",
    "            for q_term in tokenized_terms_list:\n",
    "                if result_set is None :\n",
    "                    prev_result_set =self.doc_index.inverted_index_term_proximity[field_name][q_term]\n",
    "                    result_set = set(self.doc_index.inverted_index_term_proximity[field_name][q_term].keys())\n",
    "                else :\n",
    "                    temp_result_set = set()\n",
    "                    curr_result_set = self.doc_index.inverted_index_term_proximity[field_name][q_term]\n",
    "                    for doc_id in QueryTermMode.AND(result_set, set(curr_result_set.keys())):\n",
    "                        positions_current_term =curr_result_set[doc_id]\n",
    "                        positions_previous_term =prev_result_set[doc_id]\n",
    "                        for tc_position in positions_current_term :\n",
    "                            for tp_position in positions_previous_term:\n",
    "                                tp_tc =tc_position - tp_position\n",
    "                                if (tp_tc>0 and tp_tc<=max_separation_between_terms ):\n",
    "                                    temp_result_set.add(doc_id)\n",
    "                                    break\n",
    "                    result_set =temp_result_set\n",
    "                    prev_result_set = curr_result_set             \n",
    "        return result_set\n",
    "\n",
    "    \n",
    "    def term_search(self, query_terms_as_string, q_mode=QueryTermMode.OR):\n",
    "        \"\"\"\n",
    "        search for query string in all fields\n",
    "        Returns set of document/product ids \n",
    "        \"\"\"\n",
    "        result_set=None\n",
    "        for field_name, tokenizer  in self.doc_index.field_names.items():   \n",
    "            tokenized_text = tokenizer(query_terms_as_string)\n",
    "            result_set   = self.field_term_search (field_name, tokenized_text, q_mode=q_mode) if result_set is None else QueryTermMode.OR(result_set, self.field_term_search (field_name, tokenized_text, q_mode=q_mode))   \n",
    "        \n",
    "        return result_set\n",
    "    \n",
    "    def ranked_search(self, field_name, query_terms_as_string, length, ranking_function):\n",
    "        \"\"\"Returns the `size` most relevant documents based on the `query`\"\"\"\n",
    "        scores = ranking_function(field_name, query_terms_as_string)\n",
    "        scores=sorted(scores,  key = lambda x: (-scores[x], x))\n",
    "        #indexes= list(scores.keys())\n",
    "        return scores[:length]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mowonibi\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nltk.download('stopwords')\n",
    "inv_ind = Inverted_Index()\n",
    "inv_ind.index_texts(include_doc2vec=True, index_doc_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ind.field_dict['title'].save(\"title\" +'_dictionary.gensim')\n",
    "#dictionary.save(field_name +'_dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sso =Searcher(inv_ind)\n",
    "ssss = sso.term_search(\"android iphone ipad\", QueryTermMode.AND)\n",
    "print(len(ssss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_alg = RankFunction.lda_similarity(inv_ind)\n",
    "ranking_alg2 = RankFunction.BM25_score(inv_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2354, 2364, 2366, 2369, 2374, 2384, 2385, 2388, 2395, 2398]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sso.ranked_search(\"description\", \"android iphone ipad\", 10,ranking_alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75814, 75831, 93734, 51592, 63858, 105597, 56691, 13010, 32735, 123506]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sso.ranked_search(\"description\", \"android iphone ipad\", 10,ranking_alg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Part of the connected lifestyle range from Netatmo, the Weather Station for Smartphones has an indoor and outdoor module so you can effectively adapt you lifestyle according to the environment. Indoor module Place this module anywhere inside the home to maximise your family's comfort. Using the app you can access: indoor temperature, relative humidity, indoor air quality, C02 readings and even a sound meter so you can live in a healthier home. Outdoor module This module means that you can plan your outdoor activities in accordance to real-time weather readings. With the app, you can access data to: outdoor temperature, outdoor relative humidity, outdoor air quality, barometric pressure and the weather. Analyse your data The Netatmo web app will display all data in the form of graphs. This not only allows you to observe the cycles and forecast variations around you, but you can also get a more accurate idea of your environment over time. Design With its simple, ergonomic and user-friendly design, the Weather Station can be used by the whole family when connected to the home Wi-Fi. The solid aluminium case offers suitable protection against the elements and seamlessly blends into its surroundings. Compatible with: iPhone 6 Plus, iPhone 6, iPhone 5s, iPhone 5c, iPhone 5, iPhone 4s, iPhone 4 iPad mini, iPad (4th generation), iPad (3rd generation), iPad 2, iPod touch (5th generation) Android 4 (minimum required with access to Google Play) Windows Phone 8.0 (minimum required)   What's in the box?: One Indoor Module One Outdoor Module USB power adapter USB cable 2x AAA batteries - Male\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_ind.corpus.description.values[75814:75815]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
